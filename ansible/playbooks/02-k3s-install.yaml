---
# ==============================================================================
# PLAY 1: K3s Installation (Kører på K8s Noder)
# ==============================================================================
- name: 1. Installer K3s Masters og Workers
  hosts: all
  become: yes
  gather_facts: no
  vars:
    k3s_version: "v1.34.2+k3s1" 
    
    # Argumenter til Master 1 (Cluster Init)
    # Binder til 0.0.0.0 for at lytte på alle interfaces
    k3s_master_init_args: "--cluster-init --bind-address 0.0.0.0 --advertise-address 192.168.8.101 --disable traefik --disable servicelb"
    
    # Ingen specielle argumenter til workers
    k3s_worker_args: ""

  tasks:
    # --- MASTER 1 INSTALLATION ---
    - name: 1.1 Installer K3s på Master 1 (Cluster Init)
      ansible.builtin.shell: |
        curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="{{ k3s_master_init_args }}" INSTALL_K3S_VERSION="{{ k3s_version }}" sh -
      when: inventory_hostname == 'b-k8s-master-1'
      
    # --- NYT TRIN: VENT PÅ AT MASTER 1 ER KLAR ---
    # Dette sikrer, at vi ikke prøver at joine Master 2, før Master 1 lytter.
    - name: 1.1.1 Vent på at Master 1 API er klar (Port 6443)
      ansible.builtin.wait_for:
        host: 192.168.8.101
        port: 6443
        delay: 10
        timeout: 300
      delegate_to: localhost # Tjekker fra b-admin mod Master 1
      run_once: true 
    - name: 1.2 Hent K3s Node Token fra Master 1
      ansible.builtin.shell: cat /var/lib/rancher/k3s/server/node-token
      register: k3s_token_result
      delegate_to: b-k8s-master-1 
      run_once: true

    - name: 1.3 Gem Token (Fact)
      ansible.builtin.set_fact:
        k3s_node_token: "{{ k3s_token_result.stdout }}"
      run_once: true

    # --- MASTER 2 JOIN (Simpel metode) ---
    - name: 1.4 Installer K3s på Master 2 (Join til Clusteret)
      ansible.builtin.shell: |
        curl -sfL https://get.k3s.io | K3S_URL=https://192.168.8.101:6443 K3S_TOKEN="{{ k3s_node_token }}" INSTALL_K3S_VERSION="{{ k3s_version }}" sh -s - server
      when: inventory_hostname == 'b-k8s-master-2' and k3s_token_result is defined

    # --- WORKER JOIN ---
    - name: 1.5 Installer K3s på Workers (Join til Clusteret)
      ansible.builtin.shell: |
        curl -sfL https://get.k3s.io | K3S_URL=https://192.168.8.101:6443 K3S_TOKEN="{{ k3s_node_token }}" INSTALL_K3S_VERSION="{{ k3s_version }}" INSTALL_K3S_EXEC="{{ k3s_worker_args }}" sh -s - agent
      when: inventory_hostname is in groups['k3s_workers'] and k3s_token_result is defined

    # --- SIKKERHEDS FIX: Kopiering af Kubeconfig ---
    - name: 1.6 Giv gitops bruger Læseadgang til Kubeconfig
      ansible.builtin.file:
        path: /etc/rancher/k3s/k3s.yaml
        owner: gitops
        group: gitops
        mode: '0644'
      when: inventory_hostname == 'b-k8s-master-1'
      
    - name: 1.7 Kopier Kubeconfig til Admin Node (30100)
      # Kopierer filen til Admin Noden, så jeg kan bruge den i Play 2
      ansible.builtin.fetch:
        src: /etc/rancher/k3s/k3s.yaml
        dest: /home/gitops/.kube/config-k3s
        flat: yes
        fail_on_missing: yes
      when: inventory_hostname == 'b-k8s-master-1'
      
# ==============================================================================
# PLAY 2: Cluster Hardening og Labeling (Kører fra Admin Node: localhost)
# ==============================================================================
- name: 2. Cluster Hardening og Labeling (Kørt fra Admin Node)
  hosts: localhost 
  become: no
  gather_facts: yes
  vars:
    kubeconfig_path: "/home/gitops/.kube/config-k3s"
    master_ip: "192.168.8.101"
    
  tasks:
    # --- KRITISK FIX: Ret 127.0.0.1 til 192.168.8.101 i konfigurationsfilen ---
    # Dette sikrer at Admin Noden forbinder til Master 1 og ikke sig selv
    - name: 2.0 Ret Server IP i Kubeconfig (localhost -> master IP)
      ansible.builtin.replace:
        path: "{{ kubeconfig_path }}"
        regexp: '127.0.0.1'
        replace: "{{ master_ip }}"
      
    - name: 2.0.1 Ret Server IP i Kubeconfig (Fail-safe for andre IPs)
      ansible.builtin.replace:
        path: "{{ kubeconfig_path }}"
        regexp: 'server: https://.*:6443'
        replace: "server: https://{{ master_ip }}:6443"

    # --- Nu kan jeg tjekke status ---
    - name: 2.1 Vent på at K8s API er klar (Localhost check)
      ansible.builtin.wait_for:
        host: "{{ master_ip }}"
        port: 6443
        timeout: 300
      delegate_to: localhost

    - name: 2.2 Vent på at alle 4 Noder er Ready
      # Bruger KUBECONFIG miljøvariabel for at omgå flag-problemer
      ansible.builtin.shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        kubectl get nodes | grep -v 'NotReady' | grep -v 'NAME' | wc -l
      register: nodes_ready
      until: nodes_ready.stdout.strip() | int == 4 
      retries: 40 
      delay: 10
      delegate_to: localhost

    - name: 2.3 Anvend Topology Label (Fysisk Host)
      ansible.builtin.shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        kubectl label node {{ item.0 }} topology.kubernetes.io/zone={{ item.1 }} --overwrite
      loop:
        - [ "b-k8s-master-1", "{{ hostvars['b-k8s-master-1']['pve_host'] }}" ]
        - [ "b-k8s-master-2", "{{ hostvars['b-k8s-master-2']['pve_host'] }}" ]
        - [ "b-k8s-worker-1", "{{ hostvars['b-k8s-worker-1']['pve_host'] }}" ]
        - [ "b-k8s-worker-2", "{{ hostvars['b-k8s-worker-2']['pve_host'] }}" ]
      delegate_to: localhost

    - name: 2.4 Taint Master Noder (Kun til Kontrol Plan)
      ansible.builtin.shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        kubectl taint node {{ item }} node-role.kubernetes.io/control-plane=true:NoSchedule --overwrite
      loop:
        - b-k8s-master-1
        - b-k8s-master-2
      delegate_to: localhost

    - name: 2.5 Kopier Kubeconfig til standard sti for kubectl
      ansible.builtin.copy:
        src: "{{ kubeconfig_path }}"
        dest: "{{ ansible_env.HOME }}/.kube/config"
        remote_src: yes
        mode: '0600'
        owner: gitops
      delegate_to: localhost